# Benchmark globals default vaules, can be overwritten later

# Make sure we already have the default user and home path before continuing
[ ! "$userAloja" ] || [ ! "$homePrefixAloja" ] && die "userAloja or homePrefixAloja not set"

# Default benchmark to exectute and options
[ ! "$EXEC_TYPE" ] && EXEC_TYPE="default"
[ ! "$BENCH_SUITE" ] && BENCH_SUITE="Hadoop-Examples" #"HiBench2"
[ ! "$SAVE_BENCH" ] && SAVE_BENCH=""

# Path and folders
[ ! "$BENCH_SHARE_DIR" ] && BENCH_SHARE_DIR="$homePrefixAloja/$userAloja/share" #source dir usually shared over the net
[ ! "$BENCH_LOCAL_DIR" ] && BENCH_LOCAL_DIR="/scratch/local" #local temp dir for writting
[ ! "$BENCH_FOLDER" ] && BENCH_FOLDER="aloja-bench" #name for the main folder
[ ! "$BENCH_SOURCE_DIR" ] && BENCH_SOURCE_DIR="$BENCH_LOCAL_DIR/aplic" #where to get the binaries
[ ! "$BENCH_SAVE_PREPARE_LOCATION" ] && BENCH_SAVE_PREPARE_LOCATION="$BENCH_LOCAL_DIR/aloja-bench_prepare" #where to save prepare bench output
#[ ! "$BENCH_SAVE_PREPARE_LOCATION" ] && BENCH_SAVE_PREPARE_LOCATION="$$BENCH_LOCAL_DIR/aloja-bench_prepare" #where to save prepare bench output

# Logs
ALOJA_FORCE_COLORS="1" #Force colors on screen even when writing log files

# Data sizes and default scale factor
[ ! "$BENCH_DATA_SIZE" ] && BENCH_DATA_SIZE="100000000" #100MB, in bytes
#1000000000000 1TB
BENCH_SCALE_FACTOR="$(( BENCH_DATA_SIZE / 1000000000 ))" #in GB


# Enable to run multiple times a benchmark, it is useful to repeat a benchmark after a prepare
[ ! "$BENCH_NUM_RUNS" ] && BENCH_NUM_RUNS=""

# Set to 1 to auto import runs after exectuion e.g. for the vagrant VM
[ ! "$ALOJA_AUTO_IMPORT" ] && ALOJA_AUTO_IMPORT=""

# Default perf monitors and metrics collectors
[ ! "$BENCH_PERF_MONITORS" ] && BENCH_PERF_MONITORS="sar vmstat"
[ ! "$BENCH_PERF_INTERVAL" ] && BENCH_PERF_INTERVAL="1" #1sec.

# HW conf
[ ! "$NET" ]          && NET="ETH"
[ ! "$IFACE" ]        && IFACE="eth0"
[ ! "$DISK" ]         && DISK="HDD"
[ ! "$PORT_PREFIX" ]  && PORT_PREFIX="3" #port prefix to allow multiple copies of daemons

# SW conf

# extra parameters passed to the benchmark ie., for Hadoop -D params, for sleep the num of seconds
[ ! "$BENCH_EXTRA_CONFIG" ] && BENCH_EXTRA_CONFIG=""

# Java
[ ! "$BENCH_JAVA_HOME" ]    && BENCH_JAVA_HOME="jdk1.7.0_25"
[ ! "$BENCH_JAVA_VERSION" ] && BENCH_JAVA_VERSION="jdk1.7.0_25"
[ ! "$JAVA_XMS" ]           && JAVA_XMS="-Xms512m" #START
[ ! "$JAVA_XMX" ]           && JAVA_XMX="-Xmx1024m" #MAX
[ ! "$JAVA_AM_XMS" ]        && JAVA_AM_XMS="-Xms512m" #START
[ ! "$JAVA_AM_XMX" ]        && JAVA_AM_XMX="-Xmx1024m" #MAX

# Hadoop 1 (and some for 2)
[ ! "$HADOOP_VERSION" ] && HADOOP_VERSION="hadoop-1.2.1" #default Hadoop version
[ ! "$HADOOP_OPTS" ]    && HADOOP_OPTS="" # Extra hadoop options
[ ! "$YARN_OPTS" ]      && YARN_OPTS="" # Extra hadoop yarn options

#Name of tarball with extra jars for HADOOP_USER_CLASSPATH_FIRST from the public repo
[ ! "$HADOOP_EXTRA_JARS" ] && HADOOP_EXTRA_JARS=""

# Set the max number of maps (and reducers) to 1 per core by default
if [ ! "$MAX_MAPS" ] ; then
  if [ "$vmCores" ] ; then
    MAX_MAPS="$vmCores"
  else
    die "Numbes of cores not defined for cluster, cannot set maps automatically"
  fi
fi

[ ! "$REPLICATION" ]      && REPLICATION=1
[ ! "$BLOCK_SIZE" ]       && BLOCK_SIZE=134217728 #128MB in bytes
[ ! "$IO_FACTOR" ]        && IO_FACTOR=10
[ ! "$IO_FILE" ]          && IO_FILE=65536
[ ! "$COMPRESS_GLOBAL" ]  && COMPRESS_GLOBAL=0
[ ! "$COMPRESS_TYPE" ]    && COMPRESS_TYPE=0

#COMPRESS_GLOBAL=1
#COMPRESS_TYPE=1
#COMPRESS_CODEC_GLOBAL=org.apache.hadoop.io.compress.DefaultCodec
#COMPRESS_CODEC_GLOBAL=com.hadoop.compression.lzo.LzoCodec
#COMPRESS_CODEC_GLOBAL=org.apache.hadoop.io.compress.SnappyCodec

[ ! "$INSTRUMENTATION" ] && INSTRUMENTATION=0 #if to use extrae
[ ! "$DELETE_HDFS" ] && DELETE_HDFS="1" #if to delete current HDFS files (default)
#[ "$BENCH_LEAVE_SERVICES" ] &&  DELETE_HDFS="" # unset delete HDFS is leaving services

# Hadoop 2
[ ! "$OS_RESERVED_MEM_MB" ] && OS_RESERVED_MEM_MB="256"
# Set the max ammount of memory for the node manager from cluster specs
[ ! "$PHYS_MEM" ] && PHYS_MEM="$(printf %.$2f $(echo "($vmRAM*1024)-$OS_RESERVED_MEM_MB" | bc))"
[ ! "$AM_MB" ]  && AM_MB=768

# check if we are below YARN memory minimums
if (( "$PHYS_MEM" <= 2048 )) ; then
    CONTAINER_MIN_MB="512" #set to 512 on VMs with less than 2GB usable mem
    MAPS_MB=256
    REDUCES_MB=256
fi

[ ! "$NUM_CORES" ] && NUM_CORES="$vmCores"

[ ! "$CONTAINER_MIN_MB" ] && CONTAINER_MIN_MB=512
#[ ! "$CONTAINER_MAX_MB" ] && CONTAINER_MAX_MB=4096

# Bench specific configs
[ ! "$TPCH_SCALE_FACTOR" ] &&  TPCH_SCALE_FACTOR=2


# Default cluster capabilities
[ ! "$CLUSTER_DISKS" ] &&   CLUSTER_DISKS="HDD" #separate list with spaces
[ ! "$CLUSTER_NETS" ] &&    CLUSTER_NETS="ETH" #separate list with spaces
[ ! "$BENCH_MAX_DISKS" ] && BENCH_MAX_DISKS="8"

# Populate if needed with config folders to rsync
BENCH_CONFIG_FOLDERS=""

# SATA drives (HDDs)
BENCH_DISKS["HDD"]="$BENCH_LOCAL_DIR"

# Create automatically disk paths
for disk_number_tmp in $(seq  1 $BENCH_MAX_DISKS) ; do
  BENCH_DISKS["HD$disk_number_tmp"]="/scratch/attached/$disk_number_tmp"
done

# SSDs
BENCH_DISKS["SSD"]="/scratch/ssd/1"

for disk_number_tmp in $(seq  1 $BENCH_MAX_DISKS) ; do
  BENCH_DISKS["SS$disk_number_tmp"]="/scratch/ssd/$disk_number_tmp"
done

# Remotes
for disk_number_tmp in $(seq  1 $BENCH_MAX_DISKS) ; do
  BENCH_DISKS["RR$disk_number_tmp"]="/scratch/attached/$disk_number_tmp"
done

# Remotes with temp in local
for disk_number_tmp in $(seq  1 $BENCH_MAX_DISKS) ; do
  BENCH_DISKS["RL$disk_number_tmp"]="/scratch/attached/$disk_number_tmp"
done

# Sata with tmp in SSD
for disk_number_tmp in $(seq  1 $BENCH_MAX_DISKS) ; do
  BENCH_DISKS["HS$disk_number_tmp"]="/scratch/attached/$disk_number_tmp"
done

# Remotes with tmp in SSD
for disk_number_tmp in $(seq  1 $BENCH_MAX_DISKS) ; do
  BENCH_DISKS["RS$disk_number_tmp"]="/scratch/attached/$disk_number_tmp"
done