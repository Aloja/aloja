# Benchmark globals default vaules, can be overwritten later

# Make sure we already have the default user and home path before continuing
[ ! "$userAloja" ] || [ ! "$homePrefixAloja" ] && die "userAloja or homePrefixAloja not set"

# Path and folders
[ ! "$BENCH_BASE_DIR" ] && BENCH_BASE_DIR="$homePrefixAloja/$userAloja/share" #source dir usually shared over the net
[ ! "$BENCH_DEFAULT_SCRATCH" ] && BENCH_DEFAULT_SCRATCH="/scratch/local" #root dir for writting
[ ! "$BENCH_FOLDER" ] && BENCH_FOLDER="aloja-bench" #name for the main folder
[ ! "$BENCH_SOURCE_DIR" ] && BENCH_SOURCE_DIR="$BENCH_DEFAULT_SCRATCH/aplic" #where to get the binaries
[ ! "$BENCH_SAVE_PREPARE_LOCATION" ] && BENCH_SAVE_PREPARE_LOCATION="$BENCH_DEFAULT_SCRATCH/aloja-bench_prepare" #where to save prepare bench output

# Default benchmark to exectute and options
[ ! "$EXEC_TYPE" ] && EXEC_TYPE="default"
[ ! "$BENCH" ] && BENCH="HiBench"
[ ! "$LIST_BENCHS" ] && LIST_BENCHS="wordcount sort terasort kmeans pagerank bayes dfsioe nutchindexing hivebench"
[ ! "$SAVE_BENCH" ] && SAVE_BENCH=""

# HW conf
[ ! "$NET" ] && NET="ETH"
[ ! "$IFACE" ] && IFACE="eth0"
[ ! "$DISK" ] && DISK="HDD"
[ ! "$PORT_PREFIX" ] && PORT_PREFIX="3" #port prefix to allow multiple copies

# SW conf

# Java
[ ! "$BENCH_JAVA_HOME" ] && BENCH_JAVA_HOME="jdk1.7.0_25"
[ ! "$JAVA_XMS" ] && JAVA_XMS="-Xms512m" #START
[ ! "$JAVA_XMX" ] && JAVA_XMX="-Xmx512m" #MAX
[ ! "$JAVA_AM_XMS" ] && JAVA_AM_XMS="-Xms512m" #START
[ ! "$JAVA_AM_XMX" ] && JAVA_AM_XMX="-Xmx512m" #MAX

# Hadoop 1 (and some for 2)
[ ! "$HADOOP_VERSION" ] && HADOOP_VERSION="hadoop1"
[ ! "$BENCH_HADOOP_VERSION" ] && BENCH_HADOOP_VERSION="hadoop-1.0.3" #default Hadoop version

[ ! "$MAX_MAPS" ] && MAX_MAPS=8
[ ! "$REPLICATION" ] && REPLICATION=1

[ ! "$BLOCK_SIZE" ] && BLOCK_SIZE=67108864 #64MB in bytes

[ ! "$IO_FACTOR" ] && IO_FACTOR=10
[ ! "$IO_FILE" ] && IO_FILE=65536

[ ! "$COMPRESS_GLOBAL" ] && COMPRESS_GLOBAL=0
[ ! "$COMPRESS_TYPE" ] && COMPRESS_TYPE=0
#COMPRESS_GLOBAL=1
#COMPRESS_TYPE=1
#COMPRESS_CODEC_GLOBAL=org.apache.hadoop.io.compress.DefaultCodec
#COMPRESS_CODEC_GLOBAL=com.hadoop.compression.lzo.LzoCodec
#COMPRESS_CODEC_GLOBAL=org.apache.hadoop.io.compress.SnappyCodec

[ ! "$INSTRUMENTATION" ] && INSTRUMENTATION=0 #if to use extrae
[ ! "$DELETE_HDFS" ] && DELETE_HDFS="1" #if to delete current HDFS files (default)

# Hadoop 2
[ ! "$PHYS_MEM" ] && PHYS_MEM=$(echo "scale=4;($vmRAM*1024)-3072" | bc)
[ ! "$NUM_CORES" ] && NUM_CORES="$vmCores"
[ ! "$CONTAINER_MIN_MB" ] && CONTAINER_MIN_MB=768
[ ! "$CONTAINER_MAX_MB" ] && CONTAINER_MAX_MB=4096
[ ! "$MAPS_MB" ] && MAPS_MB=768
[ ! "$REDUCES_MB" ]  && REDUCES_MB=1536
[ ! "$AM_MB" ]  && AM_MB=1536

# Default cluster capabilities
[ ! "$CLUSTER_DISKS" ] &&   CLUSTER_DISKS="HDD"
[ ! "$CLUSTER_NETS" ] &&    CLUSTER_NETS="ETH"
[ ! "$BENCH_MAX_DISKS" ] && BENCH_MAX_DISKS="8"

#default disk paths
declare -A BENCH_DISKS #associative array

#SATA drives (HDDs)
BENCH_DISKS["HDD"]="$BENCH_DEFAULT_SCRATCH"

#create automatically disk paths
for disk_number_tmp in $(seq  1 $BENCH_MAX_DISKS) ; do
  BENCH_DISKS["HD$disk_number_tmp"]="/scratch/attached/$disk_number_tmp"
done

#SSDs
BENCH_DISKS["SSD"]="/scratch/ssd/1"

for disk_number_tmp in $(seq  1 $BENCH_MAX_DISKS) ; do
  BENCH_DISKS["SS$disk_number_tmp"]="/scratch/ssd/$disk_number_tmp"
done

#Remotes
for disk_number_tmp in $(seq  1 $BENCH_MAX_DISKS) ; do
  BENCH_DISKS["RR$disk_number_tmp"]="/scratch/attached/$disk_number_tmp"
done

#Remotes with temp in local
for disk_number_tmp in $(seq  1 $BENCH_MAX_DISKS) ; do
  BENCH_DISKS["RL$disk_number_tmp"]="/scratch/attached/$disk_number_tmp"
done

#Sata with tmp in SSD
for disk_number_tmp in $(seq  1 $BENCH_MAX_DISKS) ; do
  BENCH_DISKS["HS$disk_number_tmp"]="/scratch/attached/$disk_number_tmp"
done

#Remotes with tmp in SSD
for disk_number_tmp in $(seq  1 $BENCH_MAX_DISKS) ; do
  BENCH_DISKS["RS$disk_number_tmp"]="/scratch/attached/$disk_number_tmp"
done